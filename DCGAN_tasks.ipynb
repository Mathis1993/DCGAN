{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VrMlKHek7hZy",
        "ndvjOYG9RkjV",
        "KRNJxh4w7k3y",
        "PFlBHKPHAbAi",
        "vnzY5CQOAmUv",
        "qrq8qFibAvux",
        "hE6Df7m-B3ex",
        "t8beAgqEXe5m",
        "okAyXBGTbzD-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathis1993/DCGAN/blob/master/DCGAN_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR_ZN8qWQXAU",
        "colab_type": "text"
      },
      "source": [
        "Welcome to this small workshop on GANs. Before we really start, we have to organize our hardware (thanks, Google!) and access to our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-3S2EITvetQ",
        "colab_type": "text"
      },
      "source": [
        "## Choose Hardware Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZxelNJEv-q5",
        "colab_type": "text"
      },
      "source": [
        "To run on GPU, from the menu, select\n",
        "* Runtime -> Change Runtime Type --> Hardware Accelerator --> GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_fRYjatLYm",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjfvAoOA9GDo",
        "colab_type": "text"
      },
      "source": [
        "Before starting, we want to mount our Google Drive to have access to our data in colab.\n",
        "\n",
        "This is where our data is located. The folder should be shared with everybody. If that's not the case, the data can be found [here](https://drive.google.com/open?id=19zewI-wxU-EACyo5yWyrIErzlj-sX-1j) to be added to your drive. The full dataset can be found [here](https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKdUuLUZR1o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loE5cuRkqPWq",
        "colab_type": "text"
      },
      "source": [
        "Next, specify the directory holding our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2JbD8tDtbpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path =  \"/content/drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMp8_6aFcTaD",
        "colab_type": "text"
      },
      "source": [
        "Now, we can get started with our Deep Convolutional Generative Adversarial Network (DCGAN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBzFaLsTwmsk",
        "colab_type": "text"
      },
      "source": [
        "## DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbfk7fVBch7j",
        "colab_type": "text"
      },
      "source": [
        "We want to create a network through which we are able to generate images of faces that look as real as possible. Or, put differently, we want a network to learn the distribution of face data, to be able to sample new data (new faces) from that distribution.\n",
        "\n",
        "This tutorial is mainly inspired by [this official pytorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) by Nathan Inkawhich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY6NS8uYYmWa",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgoNXWWu1OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "#Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed) #set randoms' random seed\n",
        "torch.manual_seed(manualSeed) #set pytorch's random seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L_jTH6qYpHP",
        "colab_type": "text"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8PkWiWjC2Y",
        "colab_type": "text"
      },
      "source": [
        "The hyperparameters' values (set here and in the model section) are best practice recommendations from [this paper](https://arxiv.org/pdf/1511.06434.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tAIRlB3xgW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Root directory for dataset\n",
        "dataroot = path + \"data_workshop\"\n",
        "\n",
        "#Number of workers for dataloader\n",
        "workers = 8\n",
        "\n",
        "#size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "#Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "#Number of training epochs\n",
        "num_epochs = 30\n",
        "\n",
        "#Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "#Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "#Whether to use GPU\n",
        "gpu = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mKxu591MI04",
        "colab_type": "text"
      },
      "source": [
        "## GPU device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xv_pL6tREx4",
        "colab_type": "text"
      },
      "source": [
        "In order to use the GPU Google is offering to us, we create a pytorch device. When creating a tensor or a model later, we can do that on this device or push the object to this device. Like that, all the calculations can easily run on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0LLvUBKMKgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and gpu) else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daX9E4nWYsH_",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bducbmyuYxsh",
        "colab_type": "text"
      },
      "source": [
        "Firts, we specify the structure of our dataset. We set our image folder up in way that we can use it with Pytorch's ImageFolder Dataset-class: In a directory, we have one or more subdirectories containing the images.\n",
        "We can specify transforms here, that are called when the data is loaded. Here, we resize the images to have 64x64 pixels, we declare them to be tensors and we normalize the range of their values (which orignially are at [0,1] to [-1,1], as this is the range the generator will output later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMA5d6expIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the dataset\n",
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mQW8ZAiZvID",
        "colab_type": "text"
      },
      "source": [
        "Next, we can feed the dataset into an instance of the DataLoader-class, specifying the batch_size, if batches should be drawn randomly and how many cores should work on retrieving batches in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOiknxxZvV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf7oaFt8frJ",
        "colab_type": "text"
      },
      "source": [
        "How many batches are loaded? It might take a while until all files from the shared data folder \"arrived\" in our drive. We have 5000 images, so with a batch size of 128 that should sum up to around 40 batches. If it's less, rerun the dataloader after some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ls68_g88xbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfY96OXyaF6Z",
        "colab_type": "text"
      },
      "source": [
        "Our dataloader is iterable, so we can call next() on it to retrieve a batch of randomly selected images. We want to plot 64 of these in a grid (conveniently done with pytorch's make_grid-function) to take a peek at our data. Note that we have to swap the tensors' axes (that hold the image data) for plotting. Pytorch usually represents image data as (n_channels x height x width) while numpy does as (height x width x n_channels).\n",
        "Also, as currently our images are in a range of [-1,1], which pyplot can't work with, we need to normalize them to [0,1] again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1szx1pytvHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "#dataloader gives tuple of (data-tensor, target-tensor) with targets defaulting to 0 (as we only used one subfolder)\n",
        "#make grid on gpu, then push it to cpu so numpy can use it\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))) #normalize from [-1,1] to [0,1] again"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9gCWtJMmkgT",
        "colab_type": "text"
      },
      "source": [
        "### Weight Initialization\n",
        "\n",
        "We want to initialize the weights of convolutional and batch-norm layers with values from specific normal distributions. We can do that when initializing our models with this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8UZSRgOmoRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idL_SWkTborR",
        "colab_type": "text"
      },
      "source": [
        "### Models\n",
        "We have two physical models:\n",
        "1. The **discriminator**: A regular CNN taking a batch of images as an input, and classifying for each image if it believes it to be real or not. It outputs the probability of an image being real (in its opinion).\n",
        "\n",
        "2. The **generator**: Into this model, we feed a vector of random numbers (drawn from the standard normal distribution). These are transformed into a 3x64x64 color image. We want this image to look real (we want the generator to capture the images' distribution)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrMlKHek7hZy",
        "colab_type": "text"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YYnCLH7Iouq",
        "colab_type": "text"
      },
      "source": [
        "The discriminator is a classical classifier. We can imagine its layers processing the 3x64x64 input images to become a prediction ranging from 0-1 like this:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1y0MIUVDW9kwOeDItmGdK9LZL8vFumoCO\" width=700/>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "The discriminator is a Convolutional Neural Network (CNN) using convolutional layers. A convolution works like this (animation by Michael Plotke taken from [here](https://commons.wikimedia.org/wiki/File:3D_Convolution_Animation.gif)):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CijsOj0SLCNHy0ZQVw7Y0qnawRwFcN1N\" width=400/>\n",
        "\n",
        "\n",
        "\n",
        "We can calculate how a convolution changes its input with the following formula:\n",
        "\n",
        "$\\mathbf O = (\\frac{I - K + 2*P}{S}) + 1$\n",
        "- O = Output dimensions\n",
        "- I = Input dimensions\n",
        "- K = Kernel size (size of the filter, 3x3 pixels in the animation)\n",
        "- P = Padding (amount of padding with zeros around the image, 1 pixel in the animation)\n",
        "- S = Stride (by how many pixels we move the kernel, 1 pixel in the animation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndvjOYG9RkjV",
        "colab_type": "text"
      },
      "source": [
        "##### **Task 1: Specify the discriminator model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMPM_FjfPiGl",
        "colab_type": "text"
      },
      "source": [
        "The discriminator should have the following layers:\n",
        "1. **Block 1** \n",
        "- *Convolution 1*\n",
        "  - Channels input: ?\n",
        "  - Channels output: 64\n",
        "  - Kernel size: 4x4\n",
        "  - Stride: 2\n",
        "  - Padding: 1\n",
        "  - No Bias\n",
        "- *Leaky ReLU*\n",
        "  - negative slope of 0.2\n",
        "\n",
        "2. **Block 2**\n",
        "- *Convolution 2*\n",
        "  - Channels input: 64\n",
        "  - Channels output: 128\n",
        "  - Other parameters as in the previous one\n",
        "- *Batch Normalization*\n",
        "  - number of features: 128\n",
        "- *Leaky ReLU*\n",
        "  - negative slope of 0.2\n",
        "\n",
        "3. **Block 3**\n",
        "- *Convolution 3*\n",
        "  - Channels input: 128\n",
        "  - Channels output: 256\n",
        "  - Other parameters as in the previous one\n",
        "- *Batch Normalization*\n",
        "  - number of features: 256\n",
        "- *Leaky ReLU*\n",
        "  - negative slope of 0.2\n",
        "\n",
        "4. **Block 4**\n",
        "- *Convolution 4*\n",
        "  - Channels input: 256\n",
        "  - Channels output: 512\n",
        "  - Other parameters as in the previous one\n",
        "- *Batch Normalization*\n",
        "  - number of features: 512\n",
        "- *Leaky ReLU*\n",
        "  - negative slope of 0.2\n",
        "\n",
        "5. **Block 5**\n",
        "- *Convolution 5*\n",
        "  - Channels input: 512\n",
        "  - Channels output: 1\n",
        "  - Kernel size: 4x4\n",
        "  - Stride: ?\n",
        "  - Padding: 0\n",
        "  - No Bias\n",
        "- *Sigmoid*\n",
        "\n",
        "Please decide how many channels we put into the first convolutional layer and what stride we need in order to reach an output of nCx1x1 (using the formula above).\n",
        "\n",
        "If you get stuck on how to implement the layers, you can take a look [here](https://colab.research.google.com/github/Mathis1993/DCGAN/blob/master/Specify_Models_Pytorch.ipynb).\n",
        "\n",
        "To look up how to specify a certain layer, you can have a look in the [documentation](https://pytorch.org/docs/stable/nn.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812MRgCp4zRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    ##Your code here\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    ##Your code here\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRNJxh4w7k3y",
        "colab_type": "text"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWw-J2kOK_CJ",
        "colab_type": "text"
      },
      "source": [
        "The generator goes the other way round: It takes as input a random vector from latent space and upsamples it to be an image of 3x64x64, like this:\n",
        "\n",
        "<br>\n",
        "\n",
        "<image src=\"https://drive.google.com/uc?export=view&id=1R6krNT2BMjeFctIX3Mok4zOyeglt9HXK\" width=700>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Where we went from a whole image to one value in the discriminator, we now want to go from a point in latent space to a whole image. To do this, we can use transposed convolutions, which up-sample our input.\n",
        "\n",
        "In a regular convolution operation, we would eg use a 2x2 kernel and apply it to a 3x3 image (without padding and a stride of 1) resulting in a 2x2 feature map. Mathematically, we change the kernel into a sparse matrix and flatten the image. Then, we can multiply both matrices and get the flattened feature map, which we can reshape to a 2x2 output.\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\begin{array}{|c|c|}\n",
        "\\hline w_{00} & w_{01} \\\\\\hline\n",
        "  w_{10} & w_{11} \\\\\\hline\n",
        "\\end{array} \n",
        "conv. \n",
        "\\begin{array}{|c|c|c|}\n",
        "\\hline x_{0} & x_{1} & x_{2} \\\\\\hline\n",
        "  x_{3} & x_{4} & x_{5} \\\\\\hline\n",
        "  x_{6} & x_{7} & x_{8} \\\\\\hline\n",
        "\\end{array}\n",
        "=\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline y_{0} & y_{1} \\\\\\hline\n",
        "  y_{2} & y_{3} \\\\\\hline\n",
        "\\end{array}  \n",
        "$$\n",
        "\n",
        "$$\\begin{pmatrix} \n",
        "w_{00} & w_{01} & 0 & w_{10} & w_{11} & 0 & 0 & 0 & 0 \\\\ \n",
        "0 & w_{00} & w_{01} & 0 & w_{10} & w_{11} & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & w_{00} & w_{01} & 0 & w_{10} & w_{11} & 0 \\\\\n",
        "0 & 0 & 0 & 0 & w_{00} & w_{01} & 0 & w_{10} & w_{11} \n",
        "\\end{pmatrix} \n",
        "* \n",
        "\\begin{pmatrix} \n",
        "x_{0} \\\\ x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ x_{5} \\\\ x_{6} \\\\ x_{7} \\\\ x_{8}\\end{pmatrix} \n",
        "= \n",
        "\\begin{pmatrix} \n",
        "y_{0} \\\\ y_{1} \\\\ y_{2} \\\\ y_{3} \\\\ y_{4} \n",
        "\\end{pmatrix}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "When we use a transposed convolution, we transpose the sparse kernel matrix and multiply it a flattened input, yielding an up-sampled output. \n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\begin{pmatrix}\n",
        "w_{00} & 0 & 0 & 0 \\\\\n",
        "w_{01} & w_{00} & 0 & 0 \\\\\n",
        "0 & w_{01} & 0 & 0 \\\\\n",
        "w_{10} & 0 & w_{00} & 0 \\\\\n",
        "w_{11} & w_{10} & w_{01} & w_{00} \\\\\n",
        "0 & w_{11} & 0 & w_{01} \\\\\n",
        "0 & 0 & w_{10} & 0 \\\\\n",
        "0 & 0 & w_{11} & w_{10} \\\\\n",
        "0 & 0 & 0 & w_{11}\n",
        "\\end{pmatrix}\n",
        "*\n",
        "\\begin{pmatrix} \n",
        "a_{0} \\\\ a_{1} \\\\ a_{2} \\\\ a_{3} \\\\ a_{4} \n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix} \n",
        "b_{0} \\\\ b_{1} \\\\ b_{2} \\\\ b_{3} \\\\ b_{4} \\\\ b_{5} \\\\ b_{6} \\\\ b_{7} \\\\ b_{8}\\end{pmatrix} \n",
        "$$ \n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNQjsb_z7mQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    self.convT_block1 = nn.Sequential(\n",
        "        #100 x 1 x 1\n",
        "        nn.ConvTranspose2d(in_channels=100, out_channels=512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.convT_block2 = nn.Sequential(\n",
        "        #512 x 4 x 4\n",
        "        nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.convT_block3 = nn.Sequential(\n",
        "        #256 x 8 x 8\n",
        "        nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.convT_block4 = nn.Sequential(\n",
        "        #128 x 16 x 16\n",
        "        nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.convT_block5 = nn.Sequential(\n",
        "        #64 x 32 x 32\n",
        "        nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "        #3 x 64 x 64\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convT_block1(x)\n",
        "    x = self.convT_block2(x)\n",
        "    x = self.convT_block3(x)\n",
        "    x = self.convT_block4(x)\n",
        "    x = self.convT_block5(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFlBHKPHAbAi",
        "colab_type": "text"
      },
      "source": [
        "#### Instantiate Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzugTsAdPUBX",
        "colab_type": "text"
      },
      "source": [
        "This will also give an overview of the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-hdSkjUnX5W",
        "colab_type": "text"
      },
      "source": [
        "Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8LSl6JiAalW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "#initialize weights\n",
        "discriminator.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKV7FosFncFa",
        "colab_type": "text"
      },
      "source": [
        "Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypKOIk1IndHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator().to(device)\n",
        "\n",
        "#initialize weights\n",
        "generator.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnzY5CQOAmUv",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQmGlqqKHgz2",
        "colab_type": "text"
      },
      "source": [
        "We are using Binary Cross Entropy Loss here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9_C1GQcAtNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrq8qFibAvux",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIH6HWXMHlbI",
        "colab_type": "text"
      },
      "source": [
        "Adam optimizers with best-practice parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q90csF0AxJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6Df7m-B3ex",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzhVx2GhHu4e",
        "colab_type": "text"
      },
      "source": [
        "Let's loop over our dataset a couple of times. Each time we train the discriminator on a batch of real and a batch of fake data. The generator is trained by using the discriminator's output when fed fake data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8beAgqEXe5m",
        "colab_type": "text"
      },
      "source": [
        "##### **Task 2: Training Loop**\n",
        "\n",
        "- Calculate the actual batch size\n",
        "\n",
        "- Train the discriminator on a real batch of data\n",
        "1. Zero the gradient buffer\n",
        "2. Forward pass: Pass a batch of real images through the discriminator\n",
        "3. Calculate the loss\n",
        "4. Backward pass: Call the backpropagation on the loss to update the model weights\n",
        "\n",
        "- Train the generator\n",
        "1. Zero the gradient buffer\n",
        "2. Forward pass: Pass a batch of fake images through the discriminator\n",
        "3. Calculate the loss\n",
        "4. Backward pass: Call the backpropagation on the loss to update the model\n",
        "\n",
        "If you can't figure out the solution to some thing, you can always look at the corresponding section in this [notebook](https://colab.research.google.com/github/Mathis1993/DCGAN/blob/master/DCGAN_collapsed_view.ipynb) containing full code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0gFittpB47k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To track how the latent space is given sense\n",
        "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
        "\n",
        "#To track progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "#For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    #For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0): #at what index to start\n",
        "\n",
        "      #extract real images from list holding data and nonsense targets\n",
        "      #and push to device\n",
        "      data_real = data[0].to(device)\n",
        "\n",
        "      #calculate acutal batch size (in the last batch per epoch, it could be != our determined batch_size)\n",
        "      b_size = ##Your code here\n",
        "\n",
        "      #targets\n",
        "      label_real = torch.full(size=(b_size,), fill_value=1).to(device)\n",
        "      label_fake = torch.full(size=(b_size,), fill_value=0).to(device)\n",
        "\n",
        "      #train discriminator on real batch\n",
        "      \n",
        "      #zero gradient buffer\n",
        "      ##Your code here\n",
        "\n",
        "      #forward pass (Consider the output is of dimensions (n_batch, 1, 1, 1), so we have to collapse it to (n_batch))\n",
        "      output_d_real = ##Your code here\n",
        "      \n",
        "      #calculate loss (using our loss-function, the output and adequate labels)\n",
        "      loss_d_real = ##Your code here\n",
        "      \n",
        "      #backward pass (call the backpropagation on the loss)\n",
        "      ##Your code here\n",
        "\n",
        "      #train discriminator on fake batch\n",
        "      #Generate batch of latent vectors\n",
        "      z = torch.randn(b_size, 100, 1, 1, device=device)\n",
        "      #Generate fake image batch with the generator\n",
        "      data_fake = generator(z)\n",
        "      output_d_fake = discriminator(data_fake.detach()).view(-1) #detach fake images from generator\n",
        "      loss_d_fake = criterion(output_d_fake, label_fake)\n",
        "      loss_d_fake.backward()\n",
        "\n",
        "      #summarize loss \n",
        "      loss_d = loss_d_real + loss_d_fake\n",
        "      #update discriminator weights\n",
        "      optimizerD.step()\n",
        "\n",
        "      #train generator\n",
        "      \n",
        "      #zero gradient buffer\n",
        "      ##Your code here\n",
        "\n",
        "      #as we just updated the discriminator, pass the fake data through it again\n",
        "      #remember to collapse the output\n",
        "      output_g = ##Your code here\n",
        "      \n",
        "      #calculate loss (using our loss-function, the output and adequate labels)\n",
        "      loss_g = ##Your code here\n",
        "      \n",
        "      #backward pass (call the backpropagation on the loss)\n",
        "      ##Your code here\n",
        "      \n",
        "      #update generator weights\n",
        "      optimizerG.step()\n",
        "\n",
        "      #track losses\n",
        "      G_losses.append(loss_d.item())\n",
        "      D_losses.append(loss_g.item())\n",
        "\n",
        "      #How did D and G perform in discriminating/generating images?\n",
        "      D_x = output_d_real.mean().item() * 100\n",
        "      D_G_z1 = output_d_fake.mean().item() * 100\n",
        "      D_G_z2 = output_g.mean().item() * 100\n",
        "\n",
        "      #print information\n",
        "      if (iters % 20 == 0):\n",
        "        print(\"Epoch [{:02d}/{:02d}], Batch [{:02d}/{:02d}], Iteration {:04d}, D_loss: {:06.3f}, G_loss: {:06.3f}. Real images recognized as real: {:05.2f}%. Fake images fooled by: {:05.2f}% and {:05.2f}%.\".format(epoch+1, num_epochs, i+1, len(dataloader), iters+1, loss_d.item(), loss_g.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "      #save generated images\n",
        "      if (iters % 40 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = generator(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "      iters += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okAyXBGTbzD-",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR4N5O1s-BW4",
        "colab_type": "text"
      },
      "source": [
        "Plot the discriminator's and generator's training losses over the iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOc06O8QuY-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe41UhH6-L70",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at a batch of real and a batch of generated images. The generated images were generated in the last training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pSK25gqMHSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get batch of images\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "#Plot real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "#Plot fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1].cpu(),(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrWQ97-FT8p",
        "colab_type": "text"
      },
      "source": [
        "Generate some more images by feeding points from latent space into the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAC_hrHtEMUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = torch.randn(64, 100, 1, 1, device=device)\n",
        "with torch.no_grad():\n",
        "  fake = generator(z).detach().cpu()\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"More Fake Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(fake, padding=2, normalize=True),(1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoaxeLMt-YbH",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize the progress our GAN made in a nice animation (This one is from from training a bit longer).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1KAilGmvF22un85-XQMgSr7GhlHugJFFJ\" width=700/>\n",
        "\n",
        "Code to create your own animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmN5bqIxt8le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Animation in output is so big that colab takes too long to save, so don't output it right now\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "#HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQh7wY4gDv5C",
        "colab_type": "text"
      },
      "source": [
        "If you want to take a look at how well a GAN for faces can do, check out this [website](https://thispersondoesnotexist.com/)."
      ]
    }
  ]
}